# -*- coding: utf-8 -*-
"""Extra_Assingment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQbuD7eUUVcQ5OhFQS55jinqDqN6IA48
"""



!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://mirrors.sonic.net/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
!tar xzf spark-3.1.2-bin-hadoop3.2.tgz
!pip install -q findspark


import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop3.2"


import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

sample = spark.read.format('csv').options(inferSchema=True, header=True).load('/content/drive/MyDrive/sample_submission.csv')
test = spark.read.format('csv').options(inferSchema=True, header=True).load('/content/drive/MyDrive/test.csv')
train = spark.read.format('csv').options(inferSchema=True, header=True).load('/content/drive/MyDrive/train.csv')

sample.printSchema()
test.printSchema()
train.printSchema()

train.describe().show()
#test.columns

# 1. transform into intergers the train and test data
cat_col_names = list(filter(lambda x: 'cat' in x, train.columns))
indexed_col_names = list(map(lambda x: x + '_indexed', cat_col_names))
encoded_col_names = list(map(lambda x: x + '_encoded', cat_col_names))

encoded_col_names

#2. application of the encoder
from pyspark.ml.feature import (VectorAssembler, OneHotEncoder, StringIndexer)

indexer = StringIndexer(inputCols=cat_col_names, outputCols=indexed_col_names)
encoder = OneHotEncoder(inputCols=indexed_col_names, outputCols=encoded_col_names)

model_index = indexer.fit(train)
train_indexed = model_index.transform(train)

model_encode = encoder.fit(train_indexed)
train_encoded = model_encode.transform(train_indexed)

model_index = indexer.fit(test)
test_indexed = model_index.transform(test)

model_index = encoder.fit(test_indexed)
test_encoded = model_index.transform(test_indexed)

test_encoded.show()
train_encoded.show()

#2. add the columns numerical names to the encoded ones and application of the vector aseembler
val_col_names = list(filter(lambda x: 'cont' in x, train.columns))

for cont in val_col_names:
  encoded_col_names.append(cont)

assembler = VectorAssembler(
    inputCols=encoded_col_names, 
    outputCol='features')
train_final = assembler.transform(train_encoded)
test_final = assembler.transform(test_encoded)

#3.test that the data from kaggle has no labels and it is not possible to calculate the accuracy 
#4.its possible to use the train data and split it: 1- new_train and 2- new_test --> calculate accuracy 

new_train, new_test = train_final.randomSplit([0.7, 0.3])
new_test.show()

# 5.gradient boosted trees approach fits well
from pyspark.ml.classification import (GBTClassifier)

GBTC = GBTClassifier(labelCol='target')
GBTC_model = GBTC.fit(train_final)

gbtc_prediction = GBTC_model.transform(new_test)
gbtc_prediction.show()

#6. calculation of the accuracy of the model selected
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol='target', metricName='accuracy')
print(f'GBT: {evaluator.evaluate(gbtc_prediction)}')

test_final.show()

#7. aply the approach to the original test data 
test_prediction = GBTC_model.transform(test_final)
test_prediction
#test_prediction.select('id', 'prediction').show()

from pyspark.ml.classification import (GBTClassifier)

GBTC_model = GBTC.fit(test_final)

gbtc_prediction = GBTC_model.transform(test_final)
gbtc_prediction.show()